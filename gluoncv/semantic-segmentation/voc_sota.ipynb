{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Reproducing SoTA on Pascal VOC Dataset\n=========================================\n\nThis is a semantic segmentation tutorial for reproducing state-of-the-art results\non Pascal VOC dataset using Gluon CV toolkit.\n\nStart Training Now\n~~~~~~~~~~~~~~~~~~\n\n.. hint::\n\n    Feel free to skip the tutorial because the training script is self-complete and ready to launch.\n\n    :download:`Download Full Python Script: train.py<../../../scripts/segmentation/train.py>`\n\n    Example training command for training DeepLabV3::\n\n        # First finetuning COCO dataset pretrained model on the augmented set\n        # If you would like to train from scratch on COCO, please see deeplab_resnet101_coco.sh\n        CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset pascal_aug --model-zoo deeplab_resnet101_coco --aux --lr 0.001 --syncbn --ngpus 4 --checkname res101\n        # Finetuning on original set\n        CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset pascal_voc --model deeplab --aux --backbone resnet101 --lr 0.0001 --syncbn --ngpus 4 --checkname res101 --resume runs/pascal_aug/deeplab/res101/checkpoint.params\n\n    For more training command options, please run ``python train.py -h``\n    Please checkout the `model_zoo <../model_zoo/index.html#semantic-segmentation>`_ for training commands of reproducing the pretrained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport mxnet as mx\nfrom mxnet import gluon, autograd\nimport gluoncv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evils in the Training Details\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nState-of-the-art results [Chen17]_ [Zhao17]_ on Pascal VOC dataset are typically\ndifficult to reproduce due to the sophisticated training details.\nIn this tutorial we walk through our state-of-the-art implementation step-by-step.\n\nDeepLabV3 Implementation\n------------------------\n\nWe implemented state-of-the-art semantic segmentation model of DeepLabV3 in Gluon-CV.\nAtrous Spatial Pyramid Pooling (ASPP) is the key part of DeepLabV3 model, which is\nbuilt on top of FCN. It combines multiple scale features with different receptive\nfield sizes, by using different atrous rate of dilated convolution and incorporating\na global pooling branch with a global receptive field.\n\nThe ASPP module is defined as::\n\n    class _ASPP(nn.HybridBlock):\n        def __init__(self, in_channels, atrous_rates, norm_layer, norm_kwargs):\n            super(_ASPP, self).__init__()\n            out_channels = 256\n            b0 = nn.HybridSequential()\n            with b0.name_scope():\n                b0.add(nn.Conv2D(in_channels=in_channels, channels=out_channels,\n                                 kernel_size=1, use_bias=False))\n                b0.add(norm_layer(in_channels=out_channels, **norm_kwargs))\n                b0.add(nn.Activation(\"relu\"))\n\n            rate1, rate2, rate3 = tuple(atrous_rates)\n            b1 = _ASPPConv(in_channels, out_channels, rate1, norm_layer, norm_kwargs)\n            b2 = _ASPPConv(in_channels, out_channels, rate2, norm_layer, norm_kwargs)\n            b3 = _ASPPConv(in_channels, out_channels, rate3, norm_layer, norm_kwargs)\n            b4 = _AsppPooling(in_channels, out_channels, norm_layer=norm_layer,\n                              norm_kwargs=norm_kwargs)\n\n            self.concurent = gluon.contrib.nn.HybridConcurrent(axis=1)\n            with self.concurent.name_scope():\n                self.concurent.add(b0)\n                self.concurent.add(b1)\n                self.concurent.add(b2)\n                self.concurent.add(b3)\n                self.concurent.add(b4)\n\n            self.project = nn.HybridSequential()\n            with self.project.name_scope():\n                self.project.add(nn.Conv2D(in_channels=5*out_channels, channels=out_channels,\n                                           kernel_size=1, use_bias=False))\n                self.project.add(norm_layer(in_channels=out_channels, **norm_kwargs))\n                self.project.add(nn.Activation(\"relu\"))\n                self.project.add(nn.Dropout(0.5))\n\n        def hybrid_forward(self, F, x):\n            return self.project(self.concurent(x))\n\nDeepLabV3 model is provided in :class:`gluoncv.model_zoo.DeepLabV3`. To get\nDeepLabV3 model using ResNet50 base network for VOC dataset:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = gluoncv.model_zoo.get_deeplab (dataset='pascal_voc', backbone='resnet50', pretrained=False)\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "COCO Pretraining\n----------------\n\nCOCO dataset is an large instance segmentation dataset with 80 categories, which has 127K\ntraining images. From the training set of MS-COCO dataset, we select with\nimages containing the 20 classes shared with PASCAL dataset with more than 1,000 labeled pixels,\nresulting 92.5K images. All the other classes are marked as background. You can simply get this\ndataset using the following command:\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# image transform for color normalization\nfrom mxnet.gluon.data.vision import transforms\ninput_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n])\n\n# get the dataset\ntrainset = gluoncv.data.COCOSegmentation(split='train', transform=input_transform)\nprint('Training images:', len(trainset))\n\n# set batch_size = 2 for toy example\nbatch_size = 2\n# Create Training Loader\ntrain_data = gluon.data.DataLoader(\n    trainset, batch_size, shuffle=True, last_batch='rollover',\n    num_workers=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot an Example of generated images:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Random pick one example for visualization:\nimport random\nfrom datetime import datetime\nrandom.seed(datetime.now())\nidx = random.randint(0, len(trainset))\nimg, mask = trainset[idx]\nfrom gluoncv.utils.viz import get_color_pallete, DeNormalize\n# get color pallete for visualize mask\nmask = get_color_pallete(mask.asnumpy(), dataset='coco')\nmask.save('mask.png')\n# denormalize the image\nimg = DeNormalize([.485, .456, .406], [.229, .224, .225])(img)\nimg = np.transpose((img.asnumpy()*255).astype(np.uint8), (1, 2, 0))\n\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\n# subplot 1 for img\nfig = plt.figure()\nfig.add_subplot(1,2,1)\n\nplt.imshow(img)\n# subplot 2 for the mask\nmmask = mpimg.imread('mask.png')\nfig.add_subplot(1,2,2)\nplt.imshow(mmask)\n# display\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Direct launch command of the COCO pretraining::\n\n  CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset coco --model deeplab --aux --backbone resnet101 --lr 0.01 --syncbn --ngpus 4 --checkname res101 --epochs 30\n\nYou can also skip the COCO pretraining by getting the pretrained model::\n\n  from gluoncv import model_zoo\n  model_zoo.get_model('deeplab_resnet101_coco', pretrained=True)\n\nPascal VOC and the Augmented Set\n--------------------------------\n\nPascal VOC dataset [Everingham10]_ has 2,913 images in training and validation sets. \nThe augmented set [Hariharan15]_ has 10,582 and 1449 training and validation images.\nWe first fine-tune the COCO pretrained model on Pascal Augmentation dataset, then\nfine-tune again on Pascal VOC dataset to get the best performance.\n\nLearning Rates\n--------------\n\nWe use different learning rates for pretrained base network and the DeepLab head without\npretrained weights.\nWe enlarge the learning rate of the head by 10 times. A poly-like cosine learning rate\nscheduling strategy is used.\nThe learning rate is given by $lr = base\\_lr \\times (1-iters/niters)^{power}$. Please\ncheck https://gluon-cv.mxnet.io/api/utils.html#gluoncv.utils.LRScheduler for more details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr_scheduler = gluoncv.utils.LRScheduler(mode='poly', base_lr=0.01,\n                                         nepochs=30, iters_per_epoch=len(train_data), power=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first use the base learning rate of 0.01 to pretrain on MS-COCO dataset,\nthen we divide the base learning rate by 10 times and 100 times respectively when\nfine-tuning on Pascal Augmented dataset and Pascal VOC original dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can `Start Training Now`_.\n\nReferences\n----------\n\n.. [Chen17] Chen, Liang-Chieh, et al. \"Rethinking atrous convolution for semantic image segmentation.\" \\\n    arXiv preprint arXiv:1706.05587 (2017).\n\n.. [Zhao17] Zhao, Hengshuang, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. \\\n    \"Pyramid scene parsing network.\" IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2017.\n\n.. [Everingham10] Everingham, Mark, Luc Van Gool, Christopher KI Williams, John Winn, \\\n    and Andrew Zisserman. \"The pascal visual object classes (voc) challenge.\" \\\n    International journal of computer vision 88, no. 2 (2010): 303-338.\n\n.. [Hariharan15] Hariharan, Bharath, Pablo Arbel\u00e1ez, Ross Girshick, and Jitendra Malik. \\\n    \"Hypercolumns for object segmentation and fine-grained localization.\" In Proceedings of \\\n    the IEEE conference on computer vision and pattern recognition, pp. 447-456. 2015.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}