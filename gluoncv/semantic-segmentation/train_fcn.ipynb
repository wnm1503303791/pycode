{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train FCN on Pascal VOC Dataset\n",
    "=====================================\n",
    "\n",
    "This is a semantic segmentation tutorial using Gluon CV toolkit, a step-by-step example.\n",
    "The readers should have basic knowledge of deep learning and should be familiar with Gluon API.\n",
    "New users may first go through `A 60-minute Gluon Crash Course <http://gluon-crash-course.mxnet.io/>`_.\n",
    "You can `Start Training Now`_ or `Dive into Deep`_.\n",
    "\n",
    "Start Training Now\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    ".. hint::\n",
    "\n",
    "    Feel free to skip the tutorial because the training script is self-complete and ready to launch.\n",
    "\n",
    "    :download:`Download Full Python Script: train.py<../../../scripts/segmentation/train.py>`\n",
    "\n",
    "    Example training command::\n",
    "\n",
    "        # First training on augmented set\n",
    "        CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset pascal_aug --model fcn --backbone resnet50 --lr 0.001 --checkname mycheckpoint\n",
    "        # Finetuning on original set\n",
    "        CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset pascal_voc --model fcn --backbone resnet50 --lr 0.0001 --checkname mycheckpoint --resume runs/pascal_aug/fcn/mycheckpoint/checkpoint.params\n",
    "\n",
    "    For more training command options, please run ``python train.py -h``\n",
    "    Please checkout the `model_zoo <../model_zoo/index.html#semantic-segmentation>`_ for training commands of reproducing the pretrained model.\n",
    "\n",
    "Dive into Deep\n",
    "~~~~~~~~~~~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "\n",
    "import gluoncv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Convolutional Network\n",
    "---------------------------\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*wRkj6lsQ5ckExB5BoYkrZg.png)\n",
    "\n",
    "    :width: 70%\n",
    "    :align: center\n",
    "\n",
    "(figure credit to `Long et al. <https://arxiv.org/pdf/1411.4038.pdf>`_ )\n",
    "\n",
    "State-of-the-art approaches of semantic segmentation are typically based on\n",
    "Fully Convolutional Network (FCN) [Long15]_.\n",
    "The key idea of a fully convolutional network is that it is \"fully convolutional\",\n",
    "which means it does not have any fully connected layers. Therefore, the network can\n",
    "accept arbitrary input size and make dense per-pixel predictions.\n",
    "Base/Encoder network is typically pre-trained on ImageNet, because the features\n",
    "learned from diverse set of images contain rich contextual information, which\n",
    "can be beneficial for semantic segmentation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Dilation\n",
    "--------------\n",
    "\n",
    "The adaption of base network pre-trained on ImageNet leads to loss spatial resolution,\n",
    "because these networks are originally designed for classification task.\n",
    "Following standard implementation in recent works of semantic segmentation,\n",
    "we apply dilation strategy to the\n",
    "stage 3 and stage 4 of the pre-trained networks, which produces stride of 8\n",
    "featuremaps (models are provided in\n",
    ":class:`gluoncv.model_zoo.ResNetV1b`).\n",
    "Visualization of dilated/atrous convoution\n",
    "(figure credit to `conv_arithmetic <https://github.com/vdumoulin/conv_arithmetic>`_ ):\n",
    "\n",
    "![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif)\n",
    "\n",
    "    :width: 40%\n",
    "    :align: center\n",
    "\n",
    "Loading a dilated ResNet50 is simply:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net = gluoncv.model_zoo.resnet50_v1b(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we provide a base model for semantic segmentation, which automatically\n",
    "load the pre-trained dilated ResNet :class:`gluoncv.model_zoo.segbase.SegBaseModel`\n",
    "with a convenient method ``base_forward(input)`` to get stage 3 & 4 featuremaps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of c3 & c4 featuremaps are  (1, 1024, 28, 28) (1, 2048, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "basemodel = gluoncv.model_zoo.segbase.SegBaseModel(nclass=10, aux=False)\n",
    "x = mx.nd.random.uniform(shape=(1, 3, 224, 224))\n",
    "c3, c4 = basemodel.base_forward(x)\n",
    "print('Shapes of c3 & c4 featuremaps are ', c3.shape, c4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FCN Model\n",
    "---------\n",
    "\n",
    "We build a fully convolutional \"head\" on top of the base network,\n",
    "the FCNHead is defined as::\n",
    "\n",
    "    class _FCNHead(HybridBlock):\n",
    "        def __init__(self, in_channels, channels, norm_layer, **kwargs):\n",
    "            super(_FCNHead, self).__init__()\n",
    "            with self.name_scope():\n",
    "                self.block = nn.HybridSequential()\n",
    "                inter_channels = in_channels // 4\n",
    "                with self.block.name_scope():\n",
    "                    self.block.add(nn.Conv2D(in_channels=in_channels, channels=inter_channels,\n",
    "                                             kernel_size=3, padding=1))\n",
    "                    self.block.add(norm_layer(in_channels=inter_channels))\n",
    "                    self.block.add(nn.Activation('relu'))\n",
    "                    self.block.add(nn.Dropout(0.1))\n",
    "                    self.block.add(nn.Conv2D(in_channels=inter_channels, channels=channels,\n",
    "                                             kernel_size=1))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return self.block(x)\n",
    "\n",
    "FCN model is provided in :class:`gluoncv.model_zoo.FCN`. To get\n",
    "FCN model using ResNet50 base network for Pascal VOC dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HybridBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f549233dcd35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_FCNHead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHybridBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_FCNHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHybridSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HybridBlock' is not defined"
     ]
    }
   ],
   "source": [
    "    class _FCNHead(HybridBlock):\n",
    "        def __init__(self, in_channels, channels, norm_layer, **kwargs):\n",
    "            super(_FCNHead, self).__init__()\n",
    "            with self.name_scope():\n",
    "                self.block = nn.HybridSequential()\n",
    "                inter_channels = in_channels // 4\n",
    "                with self.block.name_scope():\n",
    "                    self.block.add(nn.Conv2D(in_channels=in_channels, channels=inter_channels,\n",
    "                                             kernel_size=3, padding=1))\n",
    "                    self.block.add(norm_layer(in_channels=inter_channels))\n",
    "                    self.block.add(nn.Activation('relu'))\n",
    "                    self.block.add(nn.Dropout(0.1))\n",
    "                    self.block.add(nn.Conv2D(in_channels=inter_channels, channels=channels,\n",
    "                                             kernel_size=1))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN(\n",
      "  (conv1): HybridSequential(\n",
      "    (0): Conv2D(3 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (5): Activation(relu)\n",
      "    (6): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "  (relu): Activation(relu)\n",
      "  (maxpool): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "  (layer1): HybridSequential(\n",
      "    (0): BottleneckV1b(\n",
      "      (conv1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu3): Activation(relu)\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckV1b(\n",
      "      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (2): BottleneckV1b(\n",
      "      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (layer2): HybridSequential(\n",
      "    (0): BottleneckV1b(\n",
      "      (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu3): Activation(relu)\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckV1b(\n",
      "      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (2): BottleneckV1b(\n",
      "      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (3): BottleneckV1b(\n",
      "      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (layer3): HybridSequential(\n",
      "    (0): BottleneckV1b(\n",
      "      (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (2): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (3): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (4): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (5): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (layer4): HybridSequential(\n",
      "    (0): BottleneckV1b(\n",
      "      (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      (relu3): Activation(relu)\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckV1b(\n",
      "      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "    (2): BottleneckV1b(\n",
      "      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu1): Activation(relu)\n",
      "      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "      (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (relu2): Activation(relu)\n",
      "      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      (relu3): Activation(relu)\n",
      "    )\n",
      "  )\n",
      "  (head): _FCNHead(\n",
      "    (block): HybridSequential(\n",
      "      (0): Conv2D(2048 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      (2): Activation(relu)\n",
      "      (3): Dropout(p = 0.1, axes=())\n",
      "      (4): Conv2D(512 -> 21, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (auxlayer): _FCNHead(\n",
      "    (block): HybridSequential(\n",
      "      (0): Conv2D(1024 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      (2): Activation(relu)\n",
      "      (3): Dropout(p = 0.1, axes=())\n",
      "      (4): Conv2D(256 -> 21, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = gluoncv.model_zoo.get_fcn(dataset='pascal_voc', backbone='resnet50', pretrained=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and Data Augmentation\n",
    "-----------------------------\n",
    "\n",
    "image transform for color normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.data.vision import transforms\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide semantic segmentation datasets in :class:`gluoncv.data`.\n",
    "For example, we can easily get the Pascal VOC 2012 dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = gluoncv.data.VOCSegmentation(split='train', transform=input_transform)\n",
    "print('Training images:', len(trainset))\n",
    "# set batch_size = 2 for toy example\n",
    "batch_size = 2\n",
    "# Create Training Loader\n",
    "train_data = gluon.data.DataLoader(\n",
    "    trainset, batch_size, shuffle=True, last_batch='rollover',\n",
    "    num_workers=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data augmentation,\n",
    "we follow the standard data augmentation routine to transform the input image\n",
    "and the ground truth label map synchronously. (*Note that \"nearest\"\n",
    "mode upsample are applied to the label maps to avoid messing up the boundaries.*)\n",
    "We first randomly scale the input image from 0.5 to 2.0 times, then rotate\n",
    "the image from -10 to 10 degrees, and crop the image with padding if needed.\n",
    "Finally a random Gaussian blurring is applied.\n",
    "\n",
    "Random pick one example for visualization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "idx = random.randint(0, len(trainset))\n",
    "img, mask = trainset[idx]\n",
    "from gluoncv.utils.viz import get_color_pallete, DeNormalize\n",
    "# get color pallete for visualize mask\n",
    "mask = get_color_pallete(mask.asnumpy(), dataset='pascal_voc')\n",
    "mask.save('mask.png')\n",
    "# denormalize the image\n",
    "img = DeNormalize([.485, .456, .406], [.229, .224, .225])(img)\n",
    "img = np.transpose((img.asnumpy()*255).astype(np.uint8), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the image and mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "# subplot 1 for img\n",
    "fig = plt.figure()\n",
    "fig.add_subplot(1,2,1)\n",
    "\n",
    "plt.imshow(img)\n",
    "# subplot 2 for the mask\n",
    "mmask = mpimg.imread('mask.png')\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(mmask)\n",
    "# display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Details\n",
    "----------------\n",
    "\n",
    "- Training Losses:\n",
    "\n",
    "    We apply a standard per-pixel Softmax Cross Entropy Loss to train FCN. For Pascal\n",
    "    VOC dataset, we ignore the loss from boundary class (number 22).\n",
    "    Additionally, an Auxiliary Loss as in PSPNet [Zhao17]_ at Stage 3 can be enabled when\n",
    "    training with command ``--aux``. This will create an additional FCN \"head\" after Stage 3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.loss import MixSoftmaxCrossEntropyLoss\n",
    "criterion = MixSoftmaxCrossEntropyLoss(aux=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning Rate and Scheduling:\n",
    "\n",
    "    We use different learning rate for FCN \"head\" and the base network. For the FCN \"head\",\n",
    "    we use $10\\times$ base learning rate, because those layers are learned from scratch.\n",
    "    We use a poly-like learning rate scheduler for FCN training, provided in :class:`gluoncv.utils.LRScheduler`.\n",
    "    The learning rate is given by $lr = base_lr \\times (1-iter)^{power}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = gluoncv.utils.LRScheduler('poly', base_lr=0.001,\n",
    "                                         nepochs=50, iters_per_epoch=len(train_data), power=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataparallel for multi-gpu training, using cpu for demo only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.utils.parallel import *\n",
    "ctx_list = [mx.cpu(0)]\n",
    "model = DataParallelModel(model, ctx_list)\n",
    "criterion = DataParallelCriterion(criterion, ctx_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create SGD solver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = mx.kv.create('device')\n",
    "optimizer = gluon.Trainer(model.module.collect_params(), 'sgd',\n",
    "                          {'lr_scheduler': lr_scheduler,\n",
    "                           'wd':0.0001,\n",
    "                           'momentum': 0.9,\n",
    "                           'multi_precision': True},\n",
    "                          kvstore = kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop\n",
    "-----------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = 0.0\n",
    "epoch = 0\n",
    "for i, (data, target) in enumerate(train_data):\n",
    "    with autograd.record(True):\n",
    "        outputs = model(data)\n",
    "        losses = criterion(outputs, target)\n",
    "        mx.nd.waitall()\n",
    "        autograd.backward(losses)\n",
    "    optimizer.step(batch_size)\n",
    "    for loss in losses:\n",
    "        train_loss += loss.asnumpy()[0] / len(losses)\n",
    "    print('Epoch %d, batch %d, training loss %.3f'%(epoch, i, train_loss/(i+1)))\n",
    "    # just demo for 2 iters\n",
    "    if i > 1:\n",
    "        print('Terminated for this demo...')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can `Start Training Now`_.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [Long15] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. \\\n",
    "    \"Fully convolutional networks for semantic segmentation.\" \\\n",
    "    Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n",
    "\n",
    ".. [Zhao17] Zhao, Hengshuang, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. \\\n",
    "    \"Pyramid scene parsing network.\" IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2017.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluoncv",
   "language": "python",
   "name": "gluoncv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
