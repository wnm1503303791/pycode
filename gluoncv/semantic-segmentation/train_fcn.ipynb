{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Train FCN on Pascal VOC Dataset\n=====================================\n\nThis is a semantic segmentation tutorial using Gluon CV toolkit, a step-by-step example.\nThe readers should have basic knowledge of deep learning and should be familiar with Gluon API.\nNew users may first go through `A 60-minute Gluon Crash Course <http://gluon-crash-course.mxnet.io/>`_.\nYou can `Start Training Now`_ or `Dive into Deep`_.\n\nStart Training Now\n~~~~~~~~~~~~~~~~~~\n\n.. hint::\n\n    Feel free to skip the tutorial because the training script is self-complete and ready to launch.\n\n    :download:`Download Full Python Script: train.py<../../../scripts/segmentation/train.py>`\n\n    Example training command::\n\n        # First training on augmented set\n        CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset pascal_aug --model fcn --backbone resnet50 --lr 0.001 --checkname mycheckpoint\n        # Finetuning on original set\n        CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --dataset pascal_voc --model fcn --backbone resnet50 --lr 0.0001 --checkname mycheckpoint --resume runs/pascal_aug/fcn/mycheckpoint/checkpoint.params\n\n    For more training command options, please run ``python train.py -h``\n    Please checkout the `model_zoo <../model_zoo/index.html#semantic-segmentation>`_ for training commands of reproducing the pretrained model.\n\nDive into Deep\n~~~~~~~~~~~~~~\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport mxnet as mx\nfrom mxnet import gluon, autograd\n\nimport gluoncv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fully Convolutional Network\n---------------------------\n\n![](https://cdn-images-1.medium.com/max/800/1*wRkj6lsQ5ckExB5BoYkrZg.png)\n\n    :width: 70%\n    :align: center\n\n(figure credit to `Long et al. <https://arxiv.org/pdf/1411.4038.pdf>`_ )\n\nState-of-the-art approaches of semantic segmentation are typically based on\nFully Convolutional Network (FCN) [Long15]_.\nThe key idea of a fully convolutional network is that it is \"fully convolutional\",\nwhich means it does not have any fully connected layers. Therefore, the network can\naccept arbitrary input size and make dense per-pixel predictions.\nBase/Encoder network is typically pre-trained on ImageNet, because the features\nlearned from diverse set of images contain rich contextual information, which\ncan be beneficial for semantic segmentation.\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Dilation\n--------------\n\nThe adaption of base network pre-trained on ImageNet leads to loss spatial resolution,\nbecause these networks are originally designed for classification task.\nFollowing standard implementation in recent works of semantic segmentation,\nwe apply dilation strategy to the\nstage 3 and stage 4 of the pre-trained networks, which produces stride of 8\nfeaturemaps (models are provided in\n:class:`gluoncv.model_zoo.ResNetV1b`).\nVisualization of dilated/atrous convoution\n(figure credit to `conv_arithmetic <https://github.com/vdumoulin/conv_arithmetic>`_ ):\n\n![](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif)\n\n    :width: 40%\n    :align: center\n\nLoading a dilated ResNet50 is simply:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pretrained_net = gluoncv.model_zoo.resnet50_v1b(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For convenience, we provide a base model for semantic segmentation, which automatically\nload the pre-trained dilated ResNet :class:`gluoncv.model_zoo.segbase.SegBaseModel`\nwith a convenient method ``base_forward(input)`` to get stage 3 & 4 featuremaps:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basemodel = gluoncv.model_zoo.segbase.SegBaseModel(nclass=10, aux=False)\nx = mx.nd.random.uniform(shape=(1, 3, 224, 224))\nc3, c4 = basemodel.base_forward(x)\nprint('Shapes of c3 & c4 featuremaps are ', c3.shape, c4.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FCN Model\n---------\n\nWe build a fully convolutional \"head\" on top of the base network,\nthe FCNHead is defined as::\n\n    class _FCNHead(HybridBlock):\n        def __init__(self, in_channels, channels, norm_layer, **kwargs):\n            super(_FCNHead, self).__init__()\n            with self.name_scope():\n                self.block = nn.HybridSequential()\n                inter_channels = in_channels // 4\n                with self.block.name_scope():\n                    self.block.add(nn.Conv2D(in_channels=in_channels, channels=inter_channels,\n                                             kernel_size=3, padding=1))\n                    self.block.add(norm_layer(in_channels=inter_channels))\n                    self.block.add(nn.Activation('relu'))\n                    self.block.add(nn.Dropout(0.1))\n                    self.block.add(nn.Conv2D(in_channels=inter_channels, channels=channels,\n                                             kernel_size=1))\n\n    def hybrid_forward(self, F, x):\n        return self.block(x)\n\nFCN model is provided in :class:`gluoncv.model_zoo.FCN`. To get\nFCN model using ResNet50 base network for Pascal VOC dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = gluoncv.model_zoo.get_fcn(dataset='pascal_voc', backbone='resnet50', pretrained=False)\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset and Data Augmentation\n-----------------------------\n\nimage transform for color normalization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mxnet.gluon.data.vision import transforms\ninput_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We provide semantic segmentation datasets in :class:`gluoncv.data`.\nFor example, we can easily get the Pascal VOC 2012 dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainset = gluoncv.data.VOCSegmentation(split='train', transform=input_transform)\nprint('Training images:', len(trainset))\n# set batch_size = 2 for toy example\nbatch_size = 2\n# Create Training Loader\ntrain_data = gluon.data.DataLoader(\n    trainset, batch_size, shuffle=True, last_batch='rollover',\n    num_workers=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For data augmentation,\nwe follow the standard data augmentation routine to transform the input image\nand the ground truth label map synchronously. (*Note that \"nearest\"\nmode upsample are applied to the label maps to avoid messing up the boundaries.*)\nWe first randomly scale the input image from 0.5 to 2.0 times, then rotate\nthe image from -10 to 10 degrees, and crop the image with padding if needed.\nFinally a random Gaussian blurring is applied.\n\nRandom pick one example for visualization:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\nfrom datetime import datetime\nrandom.seed(datetime.now())\nidx = random.randint(0, len(trainset))\nimg, mask = trainset[idx]\nfrom gluoncv.utils.viz import get_color_pallete, DeNormalize\n# get color pallete for visualize mask\nmask = get_color_pallete(mask.asnumpy(), dataset='pascal_voc')\nmask.save('mask.png')\n# denormalize the image\nimg = DeNormalize([.485, .456, .406], [.229, .224, .225])(img)\nimg = np.transpose((img.asnumpy()*255).astype(np.uint8), (1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the image and mask\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\n# subplot 1 for img\nfig = plt.figure()\nfig.add_subplot(1,2,1)\n\nplt.imshow(img)\n# subplot 2 for the mask\nmmask = mpimg.imread('mask.png')\nfig.add_subplot(1,2,2)\nplt.imshow(mmask)\n# display\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Details\n----------------\n\n- Training Losses:\n\n    We apply a standard per-pixel Softmax Cross Entropy Loss to train FCN. For Pascal\n    VOC dataset, we ignore the loss from boundary class (number 22).\n    Additionally, an Auxiliary Loss as in PSPNet [Zhao17]_ at Stage 3 can be enabled when\n    training with command ``--aux``. This will create an additional FCN \"head\" after Stage 3.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv.loss import MixSoftmaxCrossEntropyLoss\ncriterion = MixSoftmaxCrossEntropyLoss(aux=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Learning Rate and Scheduling:\n\n    We use different learning rate for FCN \"head\" and the base network. For the FCN \"head\",\n    we use $10\\times$ base learning rate, because those layers are learned from scratch.\n    We use a poly-like learning rate scheduler for FCN training, provided in :class:`gluoncv.utils.LRScheduler`.\n    The learning rate is given by $lr = base_lr \\times (1-iter)^{power}$\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr_scheduler = gluoncv.utils.LRScheduler('poly', base_lr=0.001,\n                                         nepochs=50, iters_per_epoch=len(train_data), power=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Dataparallel for multi-gpu training, using cpu for demo only\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv.utils.parallel import *\nctx_list = [mx.cpu(0)]\nmodel = DataParallelModel(model, ctx_list)\ncriterion = DataParallelCriterion(criterion, ctx_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create SGD solver\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kv = mx.kv.create('device')\noptimizer = gluon.Trainer(model.module.collect_params(), 'sgd',\n                          {'lr_scheduler': lr_scheduler,\n                           'wd':0.0001,\n                           'momentum': 0.9,\n                           'multi_precision': True},\n                          kvstore = kv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training loop\n-----------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_loss = 0.0\nepoch = 0\nfor i, (data, target) in enumerate(train_data):\n    with autograd.record(True):\n        outputs = model(data)\n        losses = criterion(outputs, target)\n        mx.nd.waitall()\n        autograd.backward(losses)\n    optimizer.step(batch_size)\n    for loss in losses:\n        train_loss += loss.asnumpy()[0] / len(losses)\n    print('Epoch %d, batch %d, training loss %.3f'%(epoch, i, train_loss/(i+1)))\n    # just demo for 2 iters\n    if i > 1:\n        print('Terminated for this demo...')\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can `Start Training Now`_.\n\nReferences\n----------\n\n.. [Long15] Long, Jonathan, Evan Shelhamer, and Trevor Darrell. \\\n    \"Fully convolutional networks for semantic segmentation.\" \\\n    Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n\n.. [Zhao17] Zhao, Hengshuang, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. \\\n    \"Pyramid scene parsing network.\" IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2017.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}