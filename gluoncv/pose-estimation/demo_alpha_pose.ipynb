{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Predict with pre-trained AlphaPose Estimation models\n==========================================================\n\nThis article shows how to play with pre-trained Alpha Pose models with only a few\nlines of code.\n\nFirst let's import some necessary libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nfrom gluoncv import model_zoo, data, utils\nfrom gluoncv.data.transforms.pose import detector_to_alpha_pose, heatmap_to_coord_alpha_pose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a pretrained model\n-------------------------\n\nLet's get a Alpha Pose model trained with input images of size 256x192 on MS COCO\ndataset. We pick the one using ResNet-101 V1b as the base model. By specifying\n``pretrained=True``, it will automatically download the model from the model\nzoo if necessary. For more pretrained models, please refer to\n:doc:`../../model_zoo/index`.\n\nNote that a Alpha Pose model takes a top-down strategy to estimate\nhuman pose in detected bounding boxes from an object detection model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "detector = model_zoo.get_model('yolo3_mobilenet1.0_coco', pretrained=True)\npose_net = model_zoo.get_model('alpha_pose_resnet101_v1b_coco', pretrained=True)\n\n# Note that we can reset the classes of the detector to only include\n# human, so that the NMS process is faster.\n\ndetector.reset_class([\"person\"], reuse_weights=['person'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pre-process an image for detector, and make inference\n--------------------\n\nNext we download an image, and pre-process with preset data transforms. Here we\nspecify that we resize the short edge of the image to 512 px. But you can\nfeed an arbitrarily sized image.\n\nThis function returns two results. The first is a NDArray with shape\n``(batch_size, RGB_channels, height, width)``. It can be fed into the\nmodel directly. The second one contains the images in numpy format to\neasy to be plotted. Since we only loaded a single image, the first dimension\nof `x` is 1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "im_fname = utils.download('https://github.com/dmlc/web-data/blob/master/' +\n                          'gluoncv/pose/soccer.png?raw=true',\n                          path='soccer.png')\nx, img = data.transforms.presets.yolo.load_test(im_fname, short=512)\nprint('Shape of pre-processed image:', x.shape)\n\nclass_IDs, scores, bounding_boxs = detector(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Process tensor from detector to keypoint network\n--------------------\n\nNext we process the output from the detector.\n\nFor a Alpha Pose network, it expects the input has the size 256x192,\nand the human is centered. We crop the bounding boxed area\nfor each human, and resize it to 256x192, then finally normalize it.\n\nIn order to make sure the bounding box has included the entire person,\nwe usually slightly upscale the box size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pose_input, upscale_bbox = detector_to_alpha_pose(img, class_IDs, scores, bounding_boxs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predict with a Alpha Pose network\n--------------------\n\nNow we can make prediction.\n\nA Alpha Pose network predicts the heatmap for each joint (i.e. keypoint).\nAfter the inference we search for the highest value in the heatmap and map it to the\ncoordinates on the original image.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_heatmap = pose_net(pose_input)\npred_coords, confidence = heatmap_to_coord_alpha_pose(predicted_heatmap, upscale_bbox)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the pose estimation results\n---------------------\n\nWe can use :py:func:`gluoncv.utils.viz.plot_keypoints` to visualize the\nresults.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = utils.viz.plot_keypoints(img, pred_coords, confidence,\n                              class_IDs, bounding_boxs, scores,\n                              box_thresh=0.5, keypoint_thresh=0.2)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}