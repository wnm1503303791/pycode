{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch划分训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内容来源：https://www.cnblogs.com/gengyi/p/9906614.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets('./data/mnist/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 55000\n",
      "validation data size: 5000\n",
      "test data size: 10000\n"
     ]
    }
   ],
   "source": [
    "# 查看各样本的子集数量：\n",
    "# 训练集train样本数\n",
    "print(\"train data size:\", mnist.train.num_examples)\n",
    "# train data size:55000)\n",
    "\n",
    "# 验证集validation样本数\n",
    "print(\"validation data size:\", mnist.validation.num_examples)\n",
    "# validation data size:5000\n",
    "\n",
    "# 测试集 test 样本数\n",
    "print(\"test data size:\", mnist.test.num_examples)\n",
    "# test data size:10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3803922 , 0.37647063, 0.3019608 ,\n",
       "       0.46274513, 0.2392157 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3529412 , 0.5411765 , 0.9215687 ,\n",
       "       0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 ,\n",
       "       0.9843138 , 0.9843138 , 0.9725491 , 0.9960785 , 0.9607844 ,\n",
       "       0.9215687 , 0.74509805, 0.08235294, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54901963,\n",
       "       0.9843138 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.7411765 , 0.09019608, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.8862746 , 0.9960785 , 0.81568635,\n",
       "       0.7803922 , 0.7803922 , 0.7803922 , 0.7803922 , 0.54509807,\n",
       "       0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 ,\n",
       "       0.5019608 , 0.8705883 , 0.9960785 , 0.9960785 , 0.7411765 ,\n",
       "       0.08235294, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.32156864, 0.0509804 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.13333334,\n",
       "       0.8352942 , 0.9960785 , 0.9960785 , 0.45098042, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.32941177, 0.9960785 ,\n",
       "       0.9960785 , 0.9176471 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.32941177, 0.9960785 , 0.9960785 , 0.9176471 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4156863 , 0.6156863 ,\n",
       "       0.9960785 , 0.9960785 , 0.95294124, 0.20000002, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09803922, 0.45882356, 0.8941177 , 0.8941177 ,\n",
       "       0.8941177 , 0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9960785 , 0.94117653, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.26666668, 0.4666667 , 0.86274517,\n",
       "       0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.5568628 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.14509805, 0.73333335,\n",
       "       0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 , 0.8745099 ,\n",
       "       0.8078432 , 0.8078432 , 0.29411766, 0.26666668, 0.8431373 ,\n",
       "       0.9960785 , 0.9960785 , 0.45882356, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.4431373 , 0.8588236 , 0.9960785 , 0.9490197 , 0.89019614,\n",
       "       0.45098042, 0.34901962, 0.12156864, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.7843138 , 0.9960785 , 0.9450981 ,\n",
       "       0.16078432, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.6627451 , 0.9960785 ,\n",
       "       0.6901961 , 0.24313727, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.18823531,\n",
       "       0.9058824 , 0.9960785 , 0.9176471 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.48627454, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.9960785 , 0.9960785 ,\n",
       "       0.6509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.54509807, 0.9960785 , 0.9333334 , 0.22352943, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.8235295 , 0.9803922 , 0.9960785 ,\n",
       "       0.65882355, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.9490197 , 0.9960785 , 0.93725497, 0.22352943, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.34901962, 0.9843138 , 0.9450981 ,\n",
       "       0.3372549 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01960784,\n",
       "       0.8078432 , 0.96470594, 0.6156863 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01568628, 0.45882356, 0.27058825,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs shape: (200, 784)\n",
      "ys shape: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "print(\"xs shape:\", xs.shape)\n",
    "# xs.shape(200,784)\n",
    "print(\"ys shape:\", ys.shape)\n",
    "# ys.shape(200,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2], [1, 2]])\n",
    "y = tf.constant([[1, 1], [1, 2]])\n",
    "z = tf.add(x, y)\n",
    "print(z)\n",
    "# [[2,3],[2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True False False]]\n"
     ]
    }
   ],
   "source": [
    "A = [[1, 3, 4, 5, 6]]\n",
    "B = [[1, 3, 4, 3, 2]]\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.equal(A, B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/hello/good/boy/doiido'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.join('/hello/', 'good/boy/', 'doiido')\n",
    "# '/hello/good/boy/doiido'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前向传播过程神经网络代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释:\n",
    "\n",
    "规定网格输入节点为 （28*28）784 个，将每张图片的像素值转变为一维向量，\n",
    "隐藏层节点 500 个，因此由输入层到隐藏层的参数 w1 形状为 [784,500]；\n",
    "输出节点 10 个，因此由隐藏层到输出层的参数 w2 形状为 [500,10]；这里的10，代表数字0-9的十分类\n",
    "参数满足截断正态分布，并使用正则化，将每个参数的正则化损失加到总损失中。\n",
    "\n",
    "由输入层到隐藏层的偏置 b1 形状为长度为 500的一维数组，由隐藏层到输出层的偏置 b2 形状为长度为 10 的一维数组，初始化值为全 0。前向传播结构第一层为输入 x 与参数 w1 矩阵相乘加上偏置 b1，再经过 relu 函数，得到隐藏层输出 y1。前向传播结构第二层为隐藏层输出 y1 与参数 w2 矩阵相乘加上偏置 b2，得到输出 y。由于输出 y 要经过 softmax 函数，使其符合概率分布，故输出 y 不经过 relu 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 28*28 = 784个像素点，784像素点组成了一个一维数组\n",
    "INPUT_NODE = 784\n",
    "# 输出10个数，每个数对应的索引号出现的概率，实现了10分类\n",
    "OUTPUT_NODE = 10\n",
    "# 定义了隐藏层节点个数\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "# \n",
    "def get_weight(shape, regularizer):\n",
    "    # 在训练神经网络时，随机生成参数 w\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    # 若使用正则化，则将每个变量的正则化损失加到总损失集合 losses 中\n",
    "    if regularizer != None: tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):  \n",
    "    b = tf.Variable(tf.zeros(shape))  \n",
    "    return b\n",
    "\n",
    "# 描述从输入到输出的数据流 \n",
    "def forward(x, regularizer):\n",
    "    # 第一层、第二层参数是直接输出的。\n",
    "    # 第一层参数\n",
    "    w1 = get_weight([INPUT_NODE, LAYER1_NODE], regularizer)\n",
    "    b1 = get_bias([LAYER1_NODE])\n",
    "    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "    # 第二层参数\n",
    "    w2 = get_weight([LAYER1_NODE, OUTPUT_NODE], regularizer)\n",
    "    b2 = get_bias([OUTPUT_NODE])\n",
    "    y = tf.matmul(y1, w2) + b2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播过程神经网络代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义每轮喂入多少张图片\n",
    "BATCH_SIZE = 200\n",
    "# 最开始的学习率\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "# 学习率的衰减率\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# 正则化系数\n",
    "REGULARIZER = 0.0001\n",
    "# 训练多少轮\n",
    "STEPS = 50000\n",
    "# 滑动平均衰减率\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "# 模型保存路径\n",
    "MODEL_SAVE_PATH=\"data/model/\"\n",
    "# 模型保存的文件名\n",
    "MODEL_NAME=\"mnist_model\"\n",
    "\n",
    "def backward(mnist):\n",
    "    # x y_ 用placeholder占位\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE])\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE])\n",
    "    # 调用前向传播的程序，计算y值\n",
    "    y = forward(x, REGULARIZER)\n",
    "    # 为轮数计数器赋初值，并设定为不可训练\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义包含正则化的损失函数loss\n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "    # 定义指数衰减学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    # 定义训练过程\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 定义滑动平均\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step, ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 实例化saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # 在会话结构中初始化所有变量\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        for i in range(STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "\n",
    "def run():\n",
    "    backward(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training step(s), loss on training batch is 3.32263.\n",
      "After 1001 training step(s), loss on training batch is 0.459273.\n",
      "After 2001 training step(s), loss on training batch is 0.388798.\n",
      "After 3001 training step(s), loss on training batch is 0.372842.\n",
      "After 4001 training step(s), loss on training batch is 0.36131.\n",
      "After 5001 training step(s), loss on training batch is 0.330846.\n",
      "After 6001 training step(s), loss on training batch is 0.349259.\n",
      "After 7001 training step(s), loss on training batch is 0.344689.\n",
      "After 8001 training step(s), loss on training batch is 0.328936.\n",
      "After 9001 training step(s), loss on training batch is 0.298664.\n",
      "After 10001 training step(s), loss on training batch is 0.313234.\n",
      "After 11001 training step(s), loss on training batch is 0.313227.\n",
      "After 12001 training step(s), loss on training batch is 0.279158.\n",
      "After 13001 training step(s), loss on training batch is 0.288505.\n",
      "After 14001 training step(s), loss on training batch is 0.289478.\n",
      "After 15001 training step(s), loss on training batch is 0.277057.\n",
      "After 16001 training step(s), loss on training batch is 0.278179.\n",
      "After 17001 training step(s), loss on training batch is 0.274975.\n",
      "After 18001 training step(s), loss on training batch is 0.268224.\n",
      "After 19001 training step(s), loss on training batch is 0.270646.\n",
      "After 20001 training step(s), loss on training batch is 0.266511.\n",
      "After 21001 training step(s), loss on training batch is 0.33919.\n",
      "After 22001 training step(s), loss on training batch is 0.253631.\n",
      "After 23001 training step(s), loss on training batch is 0.257818.\n",
      "After 24001 training step(s), loss on training batch is 0.255324.\n",
      "After 25001 training step(s), loss on training batch is 0.25143.\n",
      "After 26001 training step(s), loss on training batch is 0.248355.\n",
      "After 27001 training step(s), loss on training batch is 0.246283.\n",
      "After 28001 training step(s), loss on training batch is 0.254587.\n",
      "After 29001 training step(s), loss on training batch is 0.253544.\n",
      "After 30001 training step(s), loss on training batch is 0.238455.\n",
      "After 31001 training step(s), loss on training batch is 0.235849.\n",
      "After 32001 training step(s), loss on training batch is 0.241715.\n",
      "After 33001 training step(s), loss on training batch is 0.234925.\n",
      "After 34001 training step(s), loss on training batch is 0.240111.\n",
      "After 35001 training step(s), loss on training batch is 0.239094.\n",
      "After 36001 training step(s), loss on training batch is 0.234203.\n",
      "After 37001 training step(s), loss on training batch is 0.232006.\n",
      "After 38001 training step(s), loss on training batch is 0.231657.\n",
      "After 39001 training step(s), loss on training batch is 0.234286.\n",
      "After 40001 training step(s), loss on training batch is 0.244302.\n",
      "After 41001 training step(s), loss on training batch is 0.230987.\n",
      "After 42001 training step(s), loss on training batch is 0.227639.\n",
      "After 43001 training step(s), loss on training batch is 0.230171.\n",
      "After 44001 training step(s), loss on training batch is 0.221543.\n",
      "After 45001 training step(s), loss on training batch is 0.229332.\n",
      "After 46001 training step(s), loss on training batch is 0.222041.\n",
      "After 47001 training step(s), loss on training batch is 0.227158.\n",
      "After 48001 training step(s), loss on training batch is 0.224883.\n",
      "After 49001 training step(s), loss on training batch is 0.22096.\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code above:\n",
    "\n",
    "01） 定义每轮喂入神经网络的图片数量、初始学习率、学习率衰减率、正则化系数、训练轮数、模型保存路径以及模型保存名称等相关信息。\n",
    "\n",
    "02） 在反向传播函数中，\n",
    "\n",
    "03） 读入mnist，\n",
    "\n",
    "04） 训练数据 x 和标签 y_ 进行placeholder 占位，\n",
    "\n",
    "05） 调用 mnist_forward.py 文件中 forward() 函数，并设置正则化，\n",
    "\n",
    "06） 计算训练数据集上的预测结果 y，\n",
    "\n",
    "07） 给当前计算轮数计数器赋值，并设定为不可训练类型\n",
    "\n",
    "08） 调用包含所有参数正则化损失的损失函数 loss\n",
    "\n",
    "09） 设定指数衰减学习率 learning_rate\n",
    "\n",
    "10） 使用梯度衰减算法对模型优化，降低损失函数，并定义参数的滑动平均\n",
    "\n",
    "11） with 结构中，实现所有参数初始化，每次喂入 batch_size 组（本代码为 200 组）训练数据和对应标签，循环迭代 steps 轮，并每隔 1000 轮打印出一次损失函数值信息，并将当前会话加保存到指定路径。\n",
    "\n",
    "12） 通过函数 run() ，加载指定路径下的训练数据集，并调用规定的 backward() 函数训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当训练完模型后，给神经网络模型输入测试集验证网络的准确性和泛化性。注意，所用的测试集和训练集是相互独立的。\n",
    "\n",
    "在 with 结构中，加载指定路径下的 ckpt，若模型存在，则加载出模型到当前对话，在测试数据集上进行准确率验证，并打印出当前轮数下的准确率，若模型不存在，则打印出模型不存在的提示，从而 test()函数完成。 通过主函数 main()，加载指定路径下的测试数据集，并调用规定的 test 函数，进行模型在测试集上的准确率验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from data/model/mnist_model-49001\n",
      "After 49001 training step(s), test accuracy = 0.0743\n",
      "INFO:tensorflow:Restoring parameters from data/model/mnist_model-49001\n",
      "After 49001 training step(s), test accuracy = 0.0743\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 规定程序循环间隔是 5 秒\n",
    "TEST_INTERVAL_SECS = 5\n",
    "\n",
    "def test(mnist):\n",
    "    # with 语句复现计算图\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # placeholder 为x y_占位\n",
    "        x = tf.placeholder(tf.float32, [None, INPUT_NODE])\n",
    "        y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE])\n",
    "        # 前向传播过程计算y的值\n",
    "        y = forward(x, None)\n",
    "\n",
    "        # 实例化带滑动平均的saver对象，这样所有参数在会话中被加载时将被赋值为各自的滑动平均值\n",
    "        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "        \n",
    "        # 计算准确率\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        accuracy_score_last=0\n",
    "\n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                # 判单是否有模型\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    accuracy_score_current = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                    print(\"After %s training step(s), test accuracy = %g\" % (global_step, accuracy_score_current))\n",
    "                    if accuracy_score_current==accuracy_score_last:\n",
    "                        return\n",
    "                    accuracy_score_last=accuracy_score_current\n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "                    return\n",
    "            #time.sleep(TEST_INTERVAL_SECS)\n",
    "            \n",
    "test(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在断点处暂停训练并在之后继续训练的内容见原文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
